---
title: Pandas 内存优化
date: 2018-11-30 14:10:17
categories:
- Python
tags:
- Pandas
- 内存优化
---

`Pandas` 是数据分析中常用的一个 Python 库。本文主要讲述 `Pandas` 内存优化。其中数据读入存储的部分参考资料 [简单又实用的pandas技巧：如何将内存占用降低90%](https://www.jiqizhixin.com/articles/2018-03-07-3)，该部分内容在网上有较多说明与介绍。本文更多篇幅放在 Pandas 的 DataFrame 类型中如何进行新列的添加。

<!--more-->

## <span id='1'> 问题描述 </span> ##
对数据分析师或算法工程师而言，`pandas` 是一个常用的 Python 库，经常用来进行数据读入，清洗等操作。因此，不可避免的，从文件中读入数据或将数据存入 DataFrame 类型的变量中进行处理。这里主要针对于 DataFrame 变量对象的优化，包含数据读入（即 DataFrame 类型变量初始化）及向 DataFrame 对象中添加新列或产生新变换后的 DataFrame。

## <span id='2'> 解决思路 </span> ##
此处，优化内存使用了 `pandas` 库函数之外的 `memory_profiler` 库以监控 Python 进程的内存使用情况，可以分析到每一行代码增减的内存状况，也可以以时间为横轴监控 Python 进程的内存使用情况。此处主要以监控代码行。使用了 `datetime` 模块监控函数运行时间。

## <span id='3'> 优化过程 </span> ##
该部分主要分为从文件中读入数据与数据的处理变换部分（即对应机器学习中的数据读入与数据预处理）。

### <span id='3.1'> 数据读入 </span> ###
On the way!

### <span id='3.2'> 数据预处理 </span> ###
数据分析中，将数据进行清洗变换得到一份新的数据以进行后续分析训练是常见且高频的操作，此处主要产生新数据的方式进行测试分析。
#### 模拟预处理 ###
此处代码主要模拟了两种预处理行为，一列变换为一列（如对数变换，归一化变换，异常值缺失值过滤等情况），一列变为多列（如 OneHot 编码，时间序列分解等）。为方便后续插入到 DataFrame 中，返回为 list 类型的列名与列变换后的值。
{% codeblock lang:python %}
import numpy

def generate_one(field_name, length=5000):
    col_arr = numpy.random.randint(0, 10, size=[length])
    return [field_name], [col_arr]

def generate_more_than_one(field_name, length=5000):
    col_nums = numpy.random.randint(2, 7)
    col_arr = numpy.random.randint(0, 2, size=[length, col_nums])
    result_names = list()
    result_values = list()
    for idx in range(col_nums):
        result_values.append(col_arr[:, idx])
        result_names.append("_".join([field_name, str(idx)]))
    return result_names, result_values
{% endcodeblock %} 在列数较多时，随机的方式测试效果更平均，接近于真实项目中的情况。

#### 预处理方式 ####
实现测试了两种存储预处理之后列数据的方式，分别为
* 原有 DataFrame 变量中调用 `reindex` 方法加入到原有 DataFrame 变量中；
* 原有 DataFrame 变量中逐列加入新列；
* 保存预处理后的列名与列值，构建列名到列值的映射，调用 DataFrame 初始化方法构建新变量。

代码实现如下
{% codeblock lang:python %}
import numpy
import pandas

def analog_pre_process_reindex(data_size=(5000, 300)):
    length, cols = data_size
    processed_df = pandas.DataFrame()
    processed_df["target"] = numpy.random.randint(0, 3, size=[length])
    # generate the test data length * cols include the target
    # but result maybe has more cols
    # default OneHot:Standard=1:1
    cur_field_names = processed_df.columns.tolist()
    for field_index in range(1, cols):
        field_name = str(field_index)
        if field_index % 2 == 0:
            processed_names, processed_values = generate_one(
                field_name=field_name, length=length)
        else:
            processed_names, processed_values = generate_more_than_one(
                field_name=field_name, length=length)

        cur_field_names.extend(processed_names)
        processed_df = processed_df.reindex(
                columns=cur_field_names, copy=False)
        processed_df[processed_names] = numpy.array(processed_values).T

    return processed_df

def analog_pre_process_one_to_df(data_size=(5000, 300)):
    length, cols = data_size
    processed_df = pandas.DataFrame()
    processed_df["target"] = numpy.random.randint(0, 3, size=[length])
    # generate the test data length * cols include the target
    # but result maybe has more cols
    # default OneHot:Standard=1:1
    for field_index in range(1, cols):
        field_name = str(field_index)
        if field_index % 2 == 0:
            processed_names, processed_values = generate_one(
                field_name=field_name, length=length)
        else:
            processed_names, processed_values = generate_more_than_one(
                field_name=field_name, length=length)

        for index in range(len(processed_names)):
            one_field_name = processed_names[index]
            one_field_values = processed_values[index]
            processed_df[one_field_name] = one_field_values

    return processed_df

def analog_pre_process_list(data_size=(5000, 300)):
    length, cols = data_size
    col_names = ["target"]
    col_values = [numpy.random.randint(0, 3, size=[length])]
    # generate the test data length * cols include the target
    # but result maybe has more cols
    # default OneHot:Standard=1:1
    for field_index in range(1, cols):
        field_name = str(field_index)
        if field_index % 2 == 0:
            processed_names, processed_values = generate_one(
                field_name=field_name, length=length)
        else:
            processed_names, processed_values = generate_more_than_one(
                field_name=field_name, length=length)

        col_names.extend(processed_names)
        col_values.extend(processed_values)

    fields_num = len(col_names)
    data_dict = {col_names[idx]: col_values[idx]
                 for idx in range(fields_num)}
    return pandas.DataFrame(data_dict)
{% endcodeblock %} 此处，特征数量中一列变换到一列与一列变换到多列的比例为1:1，原有 DataFrame 变量中只有目标分类列（由数据产生方式可看出为三分类数据）。

#### 测试验证 ####
测试包含时间与内存消耗两部分，时间测试函数与主函数模块如下
{% codeblock lang:python %}
from datetime import datetime

def record_time(func, data_size):
    start_time = datetime.now()
    res = func(data_size)
    end_time = datetime.now()
    return res, (end_time - start_time).total_seconds()

if __name__ == "__main__":
    for i in range(1, 5):
        print("==============================================================")
        print("DataSize is %s * %s !" % (5000*i, 300*i))
        df, cost_time = record_time(
            func=analog_pre_process_one_to_df, data_size=(5000 * i, 300 * i))
        #   func=analog_pre_process_reindex, data_size=(5000 * i, 300 * i))
        #   func=analog_pre_process_list, data_size=(5000 * i, 300 * i))
        del df
        print("CostTime = %s seconds!" % cost_time)
{% endcodeblock %}

为避免同时测试对性能的影响，此处对每种方式均单独测试运行，分布测试数据量为 (5000\*300)、(10000\*600)、(15000\*900) 及 (20000\*1200) 数据量的情况，时间代价如下（单位为秒）。

| |**analog_pre_process_reindex**|**analog_pre_process_one_to_df**|**analog_pre_process_list**|
| :------ | :------: | :------: | :------: |
|**(5000*300)**| 5.89472 | 0.292583 | 0.069674 |
|**(10000*600)**| 48.151761 | 1.250182 | 0.215701 |
|**(15000*900)**| 160.599144 | 3.279289 | 0.462308 |
|**(20000*1200)**| 763.805195 | 6.772731 | 0.771077 |

内存测试使用 `memory_profiler` 库函数，对检测函数 `func_name` 进行内存分析的使用方式如下。此处修饰符 `@profile(precision=6)` 精度为 6 位小数。
{% codeblock lang:python %}
from memory_profiler import profile

@profile(precision=6)
def func_name(param...):
    pass

{% endcodeblock %} 运行三次分别得到的结果对比如下图所示
<img src="https://blogimages-1252423296.cos.ap-beijing.myqcloud.com/blog_pictures/11%E6%9C%88/11-4-compare-1.png" width="80%"/>

<img src="https://blogimages-1252423296.cos.ap-beijing.myqcloud.com/blog_pictures/11%E6%9C%88/11-5-compare-2.png" width="80%"/>
此处时间略大于不进行内存测试时的时间，因为使用该模块进行内存监控会降低代码的运行速度，属于正常现象。

#### 分析 && 结论 ####
On the way!

### 环境说明 ###
On the way!

## 后记 ##
On the way!
